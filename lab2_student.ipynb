{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modular-buffer",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Michael Tran\"\n",
    "COLLABORATORS = \"Junnuo Zhu, Ryan Whitford, Jaden Mak\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-pharmacy",
   "metadata": {},
   "source": [
    "# Physics C170M/270M Lab 2: Naive Bayes \n",
    "\n",
    "In this lab, you will\n",
    "- Incorporate the 8 steps of machine learning \n",
    "- Calculate a small Naive Bayes problem by hand\n",
    "- Code a Gaussian Naive Bayes model to analyze the iris dataset. \n",
    "- Use scikit-learn and Naive Bayes models to make predictions\n",
    "- Application of Naive Bayes in Astrophysics: classify stars at the center of our Galaxy\n",
    "\n",
    "Reminder: save and checkpoint often!\n",
    "\n",
    "Lab Created by: Tuan Do & Bernie Boscoe\n",
    "\n",
    "Last updated by: Tuan Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-midwest",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-thriller",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c716228494a4a0ac75e0066c6e62e706",
     "grade": false,
     "grade_id": "nb_hand",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Naive Bayes exercise by hand, with machine learning language\n",
    "Bayes theorem:\n",
    "$P(A \\mid B) = \\frac{P(A, B)}{P(B)} = \\frac{P(B\\mid A) \\times P (A)}{P(B)}$\n",
    "Basically, we are trying to find probability of event A, given the event B is true. Event B is also termed as evidence.\n",
    "P(A) is the prior of A (the prior probability, i.e. Probability of event before evidence is seen). The evidence is an attribute value of an unknown instance(here, it is event B).\n",
    "P(A|B) is a posteriori probability of B, i.e. probability of event after evidence is seen.\n",
    "\n",
    "Let $(x_{1},x_{2},…,x_{p})$ be a feature vector and y be the class label corresponding to this feature vector.\n",
    "Applying Bayes’ theorem,\n",
    "\n",
    "$$P(y \\mid X) = \\frac{P(X, y)}{P(X)} = \\frac{P(X\\mid y) \\times P (y)}{P(X)}$$\n",
    "\n",
    "where X is given as $ X=(x_{1},x_{2},…,x_{p})$. \n",
    "By substituting for X and expanding using the chain rule we get,\n",
    "\n",
    "$$P(y \\mid x_{1}, x_{2},..., x_{p}) = \\frac{P(x_{1}, x_{2},..., x_{p}, y)}{P(x_{1}, x_{2},..., x_{p})} = \\frac{P(x_{1}, x_{2},..., x_{p}\\mid y) \\times P (y)}{P(x_{1}, x_{2},..., x_{p})}$$\n",
    "\n",
    "Since $ (x_{1},x_{2},…,x_{p})$ are independent of each other,\n",
    "\n",
    "$$P(y \\mid x_{1}, x_{2},..., x_{p}) = \\frac{P (y) \\times \\prod_{i=1}^{p} P(x_{i} \\mid y)}{\\prod_{i=1}^{p} P(x_{i})}$$\n",
    "\n",
    "For all entries in the dataset, the denominator does not change, it remains constant. Therefore, the denominator can be removed and a proportionality can be introduced:\n",
    "\n",
    "$$P(y \\mid x_{1}, x_{2},..., x_{p}) \\propto P (y) \\times \\prod_{i=1}^{p} P(x_{i} \\mid y)$$\n",
    "\n",
    "In our scikit learn exercise, the response variable (y) has only two outcomes, binary (e.g., yes or no / positive or negative). There could be cases where the classification could be multivariate.\n",
    "\n",
    "To complete the specification of our classifier, we adopt the MAP (Maximum A Posteriori) decision rule, which assigns the label to the class with the highest posterior.\n",
    "\n",
    "$$\\hat{y} = p(X, y) = p(y, x_{1}, x_{2},..., x_{p}) = \\operatorname*{argmax}_{k \\in \\{1,2, ...,K\\}} P (y) \\times \\prod_{i=1}^{p} P(x_{i} \\mid y)$$\n",
    "\n",
    "We calculate probability for all ‘K’ classes using the above function and take one with the maximum value to classify a new point belongs to that class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-demonstration",
   "metadata": {},
   "source": [
    "Let's look at a naive bayes by hand example, using data science jargon. The data we will use is various aspects of weather conditions and a binary outcome, whether or not one plays golf.\n",
    "\n",
    "The dataset is divided into two parts, namely, feature matrix and the response vector.\n",
    "\n",
    "The feature matrix contains all the vectors(rows) of dataset in which each vector consists of the value of dependent features. In our dataset, features are ‘Temperature’, ‘Humidity’ and ‘Weather’.\n",
    "Response vector contains the value of class variable(prediction or output) for each row of feature matrix. In our dataset, the class variable name is ‘Play golf’. Here is a list of the categorical features and the values they take, as well as the binary predicted value.\n",
    "\n",
    "|Temperature         | Weather         | Humidity   | Play Golf |\n",
    "| ------------- |-------------| ------|--------|\n",
    "| Hot   | Sunny | High |Yes|\n",
    "| Cold     | Rainy      |  Low |No\n",
    "\n",
    "Let's look at ten events and outcomes:\n",
    "\n",
    "|Temperature         | Weather         | Humidity   | Play Golf |\n",
    "| ------------- |-------------| ------|--------|\n",
    "| Hot   | Sunny | High |Yes|\n",
    "| Hot    | Rainy      |  High |Yes|\n",
    "| Hot   | Rainy | High |Yes|\n",
    "| Cold     | Sunny |  Low |No|\n",
    "| Hot   | Sunny | Low |No|\n",
    "| Cold     | Rainy|  Low |No|\n",
    "| Cold   | Sunny | High |Yes|\n",
    "| Cold     | Rainy |  Low |No|\n",
    "| Cold  | Sunny | High |No|\n",
    "| Cold     | Rainy |  Low |Yes|\n",
    "\n",
    "Just to clear, an example of a feature vector and corresponding class variable can be: (refer to 1st row of dataset)\n",
    "\n",
    "X = (Hot, Sunny , High)\n",
    "\n",
    "y = Yes\n",
    "\n",
    "So basically, P(y|X) here means, the probability of “playing golf” given that the weather conditions are “Temperature is hot”, \"sunny\", and “high humidity”.\n",
    "\n",
    "# Naive assumption\n",
    "\n",
    "Now, its time to put a naive assumption to the Bayes’ theorem, which is, independence among the features. So now, we split evidence into the independent parts.\n",
    "\n",
    "Now, if any two events A and B are independent, then,\n",
    "\n",
    "P(A,B) = P(A)P(B)\n",
    "\n",
    "So, finally, we are left with the task of calculating $P(y)$ and $P(x_i | y).$\n",
    "\n",
    "Please note that $P(y)$ is also called the class probability and $P(x_i | y)$ is called the conditional probability.\n",
    "\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i | y)$.\n",
    "\n",
    "Let us try to apply the above formula manually on our weather dataset. For this, we need to do some precomputations on our dataset.\n",
    "\n",
    "We need to find $P(x_i | y_j$) for each $x_i$ in $X$ and $y_j$ in y. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-microwave",
   "metadata": {},
   "source": [
    "Fill in the missing values, by computing the priors. (Just use a piece of paper, you will need them for a later question)\n",
    "\n",
    "<div align=\"center\">Temperature\n",
    "\n",
    "|| Yes        | No | P(yes)|P(no)\n",
    "| ------------- |-------------| ------|--------|----|\n",
    "| Hot   | 3| 1 |3/5|1/5|\n",
    "| Cold    | 2      | ?|P(Cold \\| yes)=?|P(Cold \\| no)=?|\n",
    "| Total   |5| 5 |100%|100%|\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">Weather\n",
    "\n",
    "|| Yes        | No | P(yes)|P(no)\n",
    "| ------------- |-------------| ------|--------|----|\n",
    "| Sunny   | 2| 3 |2/5|3/5|\n",
    "| Rainy   | ?     |  ?|P(rainy \\| yes)=?|P(Rainy \\| no)=?|\n",
    "| Total   |5| 5 |100%|100%|\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">Humidity\n",
    "\n",
    "|| Yes        | No | P(yes)|P(no)\n",
    "| ------------- |-------------| ------|--------|----|\n",
    "| High   | 4| 1 |4/5|1/5|\n",
    "| Low  | ?     |  ?|?|?|\n",
    "| Total   |5| 5 |100%|100%|\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-richards",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "(2 pts.) So, in the figures above, we have calculated $P(x_i | y_j)$ for each $x_i$ in $X$ and $y_j$ in $y $ manually in the tables 1-4. For example, probability of playing golf given that the temperature is cool, i.e P(temp. = cold | play golf = Yes) = 2/5.\n",
    "\n",
    "Also, we need to find class probabilities $(P(y))$. For example, P(play golf = Yes) = 5/10.\n",
    "<div align=\"center\">Play Golf \n",
    "    \n",
    "\n",
    "| Play     | P(yes)|P(no)\n",
    "| ------------- | ------|--------|\n",
    "| Yes |  5|?|\n",
    "|No | 5|5/10|\n",
    "| Total   |10| 100%|\n",
    "</div>\n",
    "\n",
    "today = (Sunny, Hot, Low)\n",
    "\n",
    "so $P(Yes|today)$ = $P(Sunny | Yes)* P(Hot | Yes) * P(Low | Yes) * P(Yes)$ (we can ignore denominator)\n",
    "= ?\n",
    "\n",
    "<b style=\"color:red\">Type in the four fractions below, and the result when you multiply them together.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-tampa",
   "metadata": {},
   "source": [
    "P(Sunny|Yes) = 2/5, From 1 - P(Sunny|No) <br> \n",
    "P(Hot|Yes) = 3/5; From 1 - P(Hot|No) <br>\n",
    "P(Low|Yes) = 1/5; From 1 - P(Low|No) <br>\n",
    "P(Yes) = 5/10; From 1 - P(No) <br>\n",
    "P(Yes|Today) = 3/125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-attendance",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "(1 pt.) Now, what is P(No | Today)?\n",
    "Next, convert these two values into probabilities by making the sum equal to 1 (normalization):  example: P(Yes|today)/ P(Yes |today) + P(No | today)\n",
    "<b style=\"color:red\">Write the probability that golf will be played today.\n",
    "How likely is it golf will be played today?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-labor",
   "metadata": {},
   "source": [
    "P(No|Today) = P(Sunny|No) * P(Hot|No) * P(Low|No) * P(No) <br>\n",
    "P(Sunny|No) = 3/5 <br>\n",
    "P(Hot|No) = 1/5 <br>\n",
    "P(Low|No) = 4/5 <br>\n",
    "P(No) = 5/10 <br>\n",
    "P(No|Today) = 6/125 <br>\n",
    "\n",
    "Normalization: <br>\n",
    "P(Yes|Today) = 3/125 / (3/125 + 6/125) = 1/3 <br>\n",
    "P(No|Today) = 1 - P(Yes|Today) = 2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-stone",
   "metadata": {},
   "source": [
    "# Iris Classification with Naive Bayes\n",
    "\n",
    "In this part of the assignment, you will again use the [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). Recall, it consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. For a reference, see the following papers:\n",
    "\n",
    "- R. A. Fisher. \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179–188, 1936.\n",
    "\n",
    "Your goal is to construct a Naive Bayes classifier model that predicts the correct class from the sepal length and sepal width features. This lab will help learn about using probability distributions in the Naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subsequent-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "#### PACKAGE IMPORTS ####\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets, model_selection\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-accounting",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a2da2df66ed877fed912efb3af4c905",
     "grade": false,
     "grade_id": "iris-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 1: Look at the big picture\n",
    "\n",
    "What is the problem you want to solve? How do you plan to use and benefit from the the machine learning model? What is your metric for success?\n",
    "\n",
    "In this exercise, we will give you the goal and the metric:\n",
    "\n",
    "**Machine Learning Goal**: classify irises based on the measurement of their petals. \n",
    "\n",
    "**Metric**: accuracy - defined as the number of correctly typed irises divided by the total number of irises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-volume",
   "metadata": {},
   "source": [
    "# Step 2: Get the data\n",
    "\n",
    "What data are available to you for training your model? How will you access and download all the data? The type and quantity of data available will often impact your choice in the subsequent stages. \n",
    "\n",
    "Scikit Learn has a bunch of pre-built machine learning datasets that are helpful for learning new models and testing. \n",
    "#### Load and prepare the data\n",
    "\n",
    "We will first read in the Iris dataset, and then split the dataset into training and test sets. The extra cell below the code solution is provided to check your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-promise",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e86532eaeebde313134051b13f6e6fe8",
     "grade": false,
     "grade_id": "cell-3c88ffebddc1ac74",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-field",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9111b1eebd82186eae99b79e32978cf",
     "grade": true,
     "grade_id": "load_iris",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (1 pt.) Load the iris dataset (hint: see how Lab 1 loaded the Iris dataset)\n",
    "# YOUR CODE HERE\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(data = np.c_[iris['data'], iris['target']], columns = iris['feature_names'] + ['target'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-graph",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cec6939f0a08889e7b8014dce1c12b78",
     "grade": false,
     "grade_id": "iris_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 3: Explore the data\n",
    "\n",
    "Discover and visualize the data to gain insight. For example, are there more representatives of certain types of data than others?\n",
    "\n",
    "\n",
    "## Question 4\n",
    "(6 pts)\n",
    "\n",
    "For this lab, please answer the following questions about your data. Use as many cells and plots as you like. \n",
    "\n",
    "<b style=\"color:red\">1. What are the features? What is the label?   \n",
    "2. Compute some summary statistics about your dataset. For example, the number of samples, mean, median, dispersion of the features.   \n",
    "3. Plot some of the features to get a sense of the data.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-thing",
   "metadata": {},
   "source": [
    "1. The features are the petal length, petal width, sepal length, and sepal width. The features are measured in centimeters (cm). The label is the iris, which is enumerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc9a75-8daf-4b32-a228-175863cad30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ddcf0c-7451-4032-8a1d-a46a42464654",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f788c23-75bd-4aa6-b374-f8e62fb4cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.quantile(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2579774-5046-4af4-b524-d233e63d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.quantile(0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082bbfc-e00f-4a73-a525-e4ed270f6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a28019-5a45-4e3e-a008-8e57529c7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7144f5-ad41-4755-b754-b5fbd23fbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "features = df.drop(\"target\", axis = 1).select_dtypes('number').dropna()\n",
    "sns.heatmap(features.corr(), annot = True, ax = ax)\n",
    "ax.set_title(\"Iris Feature Correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-nothing",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b33e634d3ff893724a30154d0d59c92",
     "grade": false,
     "grade_id": "cell-a3ff4b47d9c58d48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 4: Prepare the data for ML algorithms\n",
    "\n",
    "Machine learning algorithms often require data to be pre-processed in a certain way, such as scaling numerical values or map categories to other representations. You will also need to decide what data to use as training and as testing. You may also need to develop a strategy to deal with missing data. \n",
    "\n",
    "Use only the first two features: **sepal length** and **width** for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-antigua",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-investing",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2e501819339598290cd5a63a8a1dc03",
     "grade": true,
     "grade_id": "Iris_3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (1 pt.) Create an array called data with the sepal length and width features and\n",
    "# Create an array called targets with the target values\n",
    "# YOUR CODE HERE\n",
    "\n",
    "f_data = df[['sepal length (cm)', 'sepal width (cm)']]\n",
    "t_data = df['target'].astype('int')\n",
    "data = f_data.to_numpy()\n",
    "targets = t_data.to_numpy()\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-democracy",
   "metadata": {},
   "source": [
    "`sklearn` has a really helpful function called `train_test_split` that helps to randomly split your data and targets for training and testing. Please read the documentation on `train_test_split` before running the following cell. Note that there is an important keyword `test_size`. (Look at what random_seed can do for you as well, we will use that later.)\n",
    "\n",
    "A short summary of terminology used below:\n",
    "\n",
    "* **training** - data used in training the machine learning model\n",
    "* **test** - data used to evaluate the performance of the model. Will use this information to further refine the model. \n",
    "* **validation** - the data left aside in the beginning only to be used at the very end when you are finished with your model and want to test on data the model has never seeen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-adventure",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcea94a27e5f9a87ecc020331e0c7ca2",
     "grade": false,
     "grade_id": "cell-10a27067233e0f0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this code to randomly shuffle the data and make train and validation splits\n",
    "\n",
    "x_train_all, x_validate, y_train_all, y_validate = model_selection.train_test_split(data, targets, test_size=0.1)\n",
    "\n",
    "# now split the training data further into training and testing\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train_all, y_train_all, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of plotting the training data\n",
    "\n",
    "labels = {0: 'Iris-Setosa', 1: 'Iris-Versicolour', 2: 'Iris-Virginica'}\n",
    "label_colours = ['blue', 'orange', 'green']\n",
    "\n",
    "def plot_data(x, y, labels, colours):\n",
    "    for c in np.unique(y):\n",
    "        inx = np.where(y == c)\n",
    "        plt.scatter(x[inx, 0], x[inx, 1], label=labels[c], c=colours[c])\n",
    "    plt.title(\"Training set\")\n",
    "    plt.xlabel(\"Sepal length (cm)\")\n",
    "    plt.ylabel(\"Sepal width (cm)\")\n",
    "    plt.legend()\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_data(x_train, y_train, labels, label_colours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-vessel",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab2fcf0cf094fdc67e835f9916c96bb7",
     "grade": false,
     "grade_id": "cell-b1e9fee4b89ee638",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 5: Select a model and train it\n",
    "\n",
    "Based on the problem and the data, there are often a handful of algorithms to try. Here, experimentation and knowledge of the strengths and weaknesses of machine learning models will help you choose a model and train it.\n",
    "\n",
    "In this lab, we will specifically use the Naive Bayes model. \n",
    "### Naive Bayes classifier\n",
    "\n",
    "We will briefly review the Naive Bayes classifier model. The fundamental equation for this classifier is Bayes' rule:\n",
    "\n",
    "$$\n",
    "P(Y=y_k | X_1,\\ldots,X_d) = \\frac{P(X_1,\\ldots,X_d | Y=y_k)P(Y=y_k)}{\\sum_{k=1}^K P(X_1,\\ldots,X_d | Y=y_k)P(Y=y_k)}\n",
    "$$\n",
    "\n",
    "In the above, $d$ is the number of features or dimensions in the inputs $X$ (in our case $d=2$), and $K$ is the number of classes (in our case $K=3$). The distribution $P(Y)$ is the class prior distribution, which is a discrete distribution over $K$ classes. The distribution $P(X | Y)$ is the class-conditional distribution over inputs.\n",
    "\n",
    "The Naive Bayes classifier makes the assumption that the data features $X_i$ are conditionally independent give the class $Y$ (the 'naive' assumption). In this case, the class-conditional distribution decomposes as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(X | Y=y_k) &= P(X_1,\\ldots,X_d | Y=y_k)\\\\\n",
    "&= \\prod_{i=1}^d P(X_i | Y=y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This simplifying assumption means that we typically need to estimate far fewer parameters for each of the distributions $P(X_i | Y=y_k)$ instead of the full joint distribution $P(X | Y=y_k)$.\n",
    "\n",
    "Once the class prior distribution and class-conditional densities are estimated, the Naive Bayes classifier model can then make a class prediction $\\hat{Y}$ for a new data input $\\tilde{X} := (\\tilde{X}_1,\\ldots,\\tilde{X}_d)$ according to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{Y} &= \\text{argmax}_{y_k} P(Y=y_k | \\tilde{X}_1,\\ldots,\\tilde{X}_d) \\\\\n",
    "&= \\text{argmax}_{y_k}\\frac{P(\\tilde{X}_1,\\ldots,\\tilde{X}_d | Y=y_k)P(Y=y_k)}{\\sum_{k=1}^K P(\\tilde{X}_1,\\ldots,\\tilde{X}_d | Y=y_k)P(Y=y_k)}\\\\\n",
    "&= \\text{argmax}_{y_k} P(\\tilde{X}_1,\\ldots,\\tilde{X}_d | Y=y_k)P(Y=y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For this lab, we will use **Gaussian distributions** to describe the distribution of sepal length and width for each of the classes of irises. \n",
    "\n",
    "## 5a Compute the Priors\n",
    "First, let's compute the priors for observing either three iris species. Without more information, we'll just use the fraction of each classes in the training data as the prior on that class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {0 : 0, 1 : 0, 2 : 0} # Sets up a dictionary to keep track of relevant classifications\n",
    "tot = 0 # Normalization Const\n",
    "for x in y_train: # run through training set to find priors\n",
    "    counter[x] += 1 \n",
    "    tot += 1\n",
    "for x in counter: # Lets me know what the priors are\n",
    "    print(f\"class {x} ({labels[x]}) has a fraction of {counter[x]/tot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-gabriel",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb51c7fefc1d7f11783d1cdd432602dd",
     "grade": false,
     "grade_id": "cell-0aea4d2a4af0d02c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-wrist",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a71d21dc74ceaaa47264819825a058e",
     "grade": true,
     "grade_id": "cell-21be458428cf4065",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (1 pt.) Create an array called priors that has the prior probability of observing each class.\n",
    "# Be sure to keep track of which class belongs to which element of the array!\n",
    "# YOUR CODE HERE\n",
    "priors = []\n",
    "for x in counter:\n",
    "    priors.append(counter[x]/tot) #track priors\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-gardening",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95a5e7d020912c9c23acb0749de9136e",
     "grade": false,
     "grade_id": "cell-25e95aced7bbf634",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5b Build the Naive Bayes Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-north",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "494f0b9d2378e6cc21a2361b29f4a707",
     "grade": false,
     "grade_id": "cell-dcf7ce6df83426fe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 7\n",
    "\n",
    "<b style=\"color:red\">Build the Naive Bayes Classifier using Gaussian distributions. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "underlying-ottawa",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fbd80a2020c5ad5a71135a6153b0028",
     "grade": true,
     "grade_id": "predict",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (3 pt.) Build Classifier here\n",
    "# YOUR CODE HERE\n",
    "def g_parameters(data, tgt, feature, target): #General function to get the mean and variance of a specific feature and tgt from the testing data\n",
    "    focused_set = data.loc[data[target] == tgt] # Creates relevant dataset with desired tgt/feature\n",
    "    mean = focused_set[feature].mean()\n",
    "    var = focused_set[feature].var()\n",
    "    return mean, var\n",
    "\n",
    "def gaussian(mean, var, inp): # Gaussian function\n",
    "    return 1/(np.sqrt(var * 2 * np.pi)) * np.e**(-(inp - mean)**2/(2 * var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebbea2-25e7-417e-b520-6ded86494d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Creates a combined data set to run through my parameters function----\n",
    "targets = pd.DataFrame(pd.Series(y_train), columns = ['target']) \n",
    "features = pd.DataFrame(x_train, columns = ['sepal length (cm)', 'sepal width (cm)'])\n",
    "combined = pd.concat([targets, features], axis = 1)\n",
    "\n",
    "#----All 12 relevant parameters (2 features * 3 classes * 2 parameters required per set)----\n",
    "g_l0_mean, g_l0_var = g_parameters(combined, 0, \"sepal length (cm)\", 'target')\n",
    "g_l1_mean, g_l1_var = g_parameters(combined, 1, \"sepal length (cm)\", 'target')\n",
    "g_l2_mean, g_l2_var = g_parameters(combined, 2, \"sepal length (cm)\", 'target')\n",
    "g_w0_mean, g_w0_var = g_parameters(combined, 0, \"sepal width (cm)\", 'target')\n",
    "g_w1_mean, g_w1_var = g_parameters(combined, 1, \"sepal width (cm)\", 'target')\n",
    "g_w2_mean, g_w2_var = g_parameters(combined, 2, \"sepal width (cm)\", 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-syracuse",
   "metadata": {},
   "source": [
    "## 5c Train the model\n",
    "\n",
    "Training the model in this case is to determine the parameters of the Gaussians describing the joint probability of observing the sepal length or width with respect to the class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-passenger",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-macintosh",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f68309cd7bfd3394f359ef6dee4fd2d",
     "grade": false,
     "grade_id": "params",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "(3 pt.)<b style=\"color:red\"> Using as many cells as you need, determine the parameters for the joint probability distribution.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Combine the parameters into an array for easy/general access----\n",
    "parameters = [[g_l0_mean, g_l0_var, g_w0_mean, g_w0_var], [g_l1_mean, g_l1_var, g_w1_mean, g_w1_var], [g_l2_mean, g_l2_var, g_w2_mean, g_w2_var]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-sussex",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94e5e92b930d141f9225f83f9c4061cb",
     "grade": false,
     "grade_id": "cell-78eeef159bf7b698",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5d Test the model predictions\n",
    "\n",
    "`sklearn` comes with a lot of useful routines to compute metrics for machine learning applications. In this lab, you'll use `accuracy_score` from `sklearn.metrics` to compute the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-mediterranean",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a564e66a08eaedecf730f204e18b19a",
     "grade": false,
     "grade_id": "cell-942ef7cf834e03c9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-reach",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e4f08f3a9559513893b3c4244040ba8",
     "grade": true,
     "grade_id": "pred_test",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (1 pt.) make a array called pred that is your prediction for most likely class given your test data\n",
    "# YOUR CODE HERE\n",
    "pred = []\n",
    "curr = -1\n",
    "curr_tgt = -1\n",
    "\n",
    "#----Compares probability score from gaussian distribution naive bayes model to find most likely target----\n",
    "for x in x_test:\n",
    "    for y in range (0, 3):\n",
    "        tmp = priors[y] * gaussian(parameters[y][0], parameters[y][1], x[0]) * gaussian(parameters[y][2], parameters[y][3], x[1])\n",
    "        if tmp > curr: # Updates max score and most likely target\n",
    "            curr_tgt = y\n",
    "            curr = tmp\n",
    "    pred.append(curr_tgt) # Adds prediction, then resets model for the next iteration\n",
    "    curr = -1\n",
    "    curr_tgt = -1\n",
    "print(pred)\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line will give you your accuracy score for the test data\n",
    "score = accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-kernel",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "For this case, we will not modify our model, so we can just go ahead evaluate the accuracy of the validation data that you saved earlier. \n",
    "\n",
    "<b style=\"color:red\">Below, find the accuracy of the validation data. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1pt.) determine the accuracy score of your validation data\n",
    "validate = []\n",
    "curr = -1\n",
    "curr_tgt = -1\n",
    "for x in x_validate:\n",
    "    for y in range (0, 3):\n",
    "        tmp = priors[y] * gaussian(parameters[y][0], parameters[y][1], x[0]) * gaussian(parameters[y][2], parameters[y][3], x[1])\n",
    "        if tmp > curr:\n",
    "            curr_tgt = y\n",
    "            curr = tmp\n",
    "    validate.append(curr_tgt)\n",
    "    curr = -1\n",
    "    curr_tgt = -1\n",
    "score = accuracy_score(y_validate, validate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-prerequisite",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b0ecd60b99583c08e929e8d798928ef",
     "grade": false,
     "grade_id": "end_iris",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Congratulations! You have now completed the iris classification. In this iris classification, we won't go through Steps 6 to 8 of the machine learning workflow, but we will in the next part of this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-resort",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45019634e98c451dc51995706d2fc500",
     "grade": false,
     "grade_id": "cell-b5df12083ed3cdaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Classifying Stars at the Galactic Center\n",
    "\n",
    "The Galactic center contains a complex population of stars. Multiple generations of star formation have occured, including as recently as 4 to 6 million years ago. The most reliable way to determine the ages of the stars is measure their spectra to examine their spectral features. Generally stars near the supermassive black hole in Galactic center are old (> 1 billion years) or young (4-6 million years). The stars that are bright enough for us to see are young stars and massive (> 10 Msun) with high surface temperatures (>20,000K), or old stars in their red giant phase with lower mass (~1 Msun) and cool temperatures (3000 to 4000 K). This difference in temperature leads to very different spectral features. \n",
    "\n",
    "In the spectra below, there are two examples of young stars and one example of an old star. The young star spectra are classified as 'early-type'. Because the temperature is high, many electrons have been ionized from atoms in its atmosphere so few absorption lines remain. The most prominent absorption line for early-type stars in this wavelength region is from hydrogen (specifically, the Brackett gamma line). Sometimes, the temperature is so high that even the electron in hydrogen is ionized, so there is very little hydrogen absorption. For the old red-giant stars, the temperatures are fairly low, so there are now many atomic absorption lines. The strongest lines in this wavelength range are from sodium (Na) atoms. In addition, because the temperature is cool, the hydrogen lines become less prominant because there is not enough high energy photons that are available to make those absorption lines. \n",
    "\n",
    "![gc_spectra](gc_spectra.png)\n",
    "\n",
    "## Feature Engineering: Equivalent Widths and Cross-Correlation\n",
    "\n",
    "To simplify and automate the process of classifying stars, we can reduce the spectrum (flux measurements at different wavelengths) into a more compact representation that hopefully still carries a lot of the information necessary for classification. In this case, we pick the strongest spectral lines (Br gamma and Na) and turn them into features for our machine learning models. We will use the concept of equilvalent width, which is related to the depth of the absorption features. In the figure above, the shaded blue regions represent the equivalent widths of the lines. The stronger the lines, the large the equivalent widths. For more details see: https://en.wikipedia.org/wiki/Equivalent_width. We will define positive equivalent widths as absorption lines (like in the figure above), and negative equivalent widths as emission lines. \n",
    "\n",
    "We will also use the cross-correlation coefficient of the spectra with templates as a potential feature. This feature is the maximum from cross-correlating a template young star and old star spectrum with the observed spectrum. You can think of this correlation coefficient as how closely matched the observed spectrum is with a template spectrum. The correlation in principle should go from 0 to 1. With 0 meaning it is not at all alike and 1 being an identical match. The correlation coefficent given in the data for this lab varies widely because the observed spectra often have more noise the the spectrum above and we only cross correlate the observed spectra with one model young star and one model old star. There are also intrinsic variations in the spectra that can also reduce the correlations. \n",
    "\n",
    "The dataset you'll be using has the equivalent widths and cross correlation coefficient measured using an automated method for spectra that have first been classified by humans, so there are labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-robin",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15a1244d1f3b90b8ce76c5255ac4fc8a",
     "grade": false,
     "grade_id": "cell-991cc712d71090d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 1: Look at the big picture\n",
    "\n",
    "What is the problem you want to solve? How do you plan to use and benefit from the the machine learning model? What is your metric for success?\n",
    "\n",
    "In this lab, we will give you the goal and the metric\n",
    "\n",
    "**Lab goal**: classify stars at the Galactic center into young or old. \n",
    "\n",
    "**Metric**: accuracy - defined as the number of correctly typed stars divided by the total number of stars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-seating",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14607ff6878f1ceb77c0161bd92c9e2c",
     "grade": false,
     "grade_id": "cell-f5cb78d5cb702b82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 2: Get the data\n",
    "\n",
    "What data are available to you for training your model? How will you access and download all the data? The type and quantity of data available will often impact your choice in the subsequent stages. \n",
    "\n",
    "Here, the data is straightforward. We have provided you with the training data in the file ``galactic_center_stars_training.csv``\n",
    "\n",
    "In the training data, there should be 8 columns: Na equivalent width, Na equivalent width error, correlation coefficient with an old star template, Br gamma equivalent width, Br gamma equivalent width uncertainty, signal-to-noise ratio of the spectrum, and young or old. The units of equilvalent widths and their uncertainties are in Angstroms. The correlation coefficients are unit-less. \n",
    "\n",
    "There is also a file called ``galactic_center_stars_eval.csv`` that has no labels. We'll be using that file to grade this assignment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-montana",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "magnetic-package",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1f4226e9fcf00f162eede69d9c6695f",
     "grade": true,
     "grade_id": "cell-7edcc8e063cf41ad",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Na  Na_err   Corr    Br  Br_err  Yng_corr    SNR   Type\n",
       "0    1.11    0.91   0.81  2.57    0.53      0.54  17.69    old\n",
       "1    5.40    0.78   0.77  0.98    0.26      0.36  20.74    old\n",
       "2    5.71    0.80   0.81  0.96    0.22      0.34  20.23    old\n",
       "3    2.54    1.45   0.93  2.94    0.87      0.11  10.92    old\n",
       "4    7.13    1.42   0.83  1.33    0.46      6.28  11.13    old\n",
       "..    ...     ...    ...   ...     ...       ...    ...    ...\n",
       "414  0.16    0.59   0.44  1.26    0.25      0.48  27.61  young\n",
       "415  1.78    0.68   0.74  1.01    0.22      0.49  23.98  young\n",
       "416 -0.14    2.11  33.54  7.66    1.57      0.42   7.38  young\n",
       "417 -0.14    1.14  30.93  3.47    0.59      0.64  13.94  young\n",
       "418  0.74    1.04   0.18  3.26    0.87      0.42  15.39  young\n",
       "\n",
       "[417 rows x 8 columns]>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1 pt.) In this cell, load the training data and the data for grading into two pandas data frames. \n",
    "# and check that they are there by looking at each head.\n",
    "# YOUR CODE HERE\n",
    "path = \"galactic_center_stars_training.csv\"\n",
    "df = pd.read_csv(path).dropna() # We do not want to be training on NaN data\n",
    "\n",
    "\n",
    "df.head\n",
    "#df_eval.head()\n",
    "\n",
    "#raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-county",
   "metadata": {},
   "source": [
    "# Step 3: Explore the data\n",
    "\n",
    "Discover and visualize the data to gain insight. For example, how complete is the dataset? Are there more representatives of certain types of data than others?\n",
    "\n",
    "\n",
    "## Question 12\n",
    "(6 pts)\n",
    "\n",
    "For this lab, please answer the following questions about your data. Use as many cells and plots as you like. \n",
    "\n",
    "<b style=\"color:red\">1. What are the features? What is the label?   \n",
    "2. Compute some summary statistics about your dataset. For example, the number of samples, mean, median, dispersion of the features.   \n",
    "3. Plot some of the features to get a sense of the data. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a74b2-2312-47b5-b133-6422397c6ee6",
   "metadata": {},
   "source": [
    "1) The features are the Na and Br gamma spectral line widths and the label is the age descriptor of the star (old or young)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05e40c-cff2-486b-8ac8-150696c9292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" The number of samples in the training data set is {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use as many cells as you need to explore the data. We will be grading manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc36860-d181-43f3-a565-7c713baf13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" The mean of the Na spectral line width is {df['Na'].mean()}\")\n",
    "print(f\" The median of the Na spectral line width is {df['Na'].median()}\")\n",
    "print(f\" The first quantile of the Na spectral line width is {df['Na'].quantile(0.25)}\")\n",
    "print(f\" The thrid quantile of the Na spectral line width is {df['Na'].quantile(0.75)}\")\n",
    "print(f\" The variance of the Na spectral line width is {df['Na'].var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05031c6-641f-46f1-ae01-7c0eaecf5755",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" The mean of the Br spectral line width is {df['Br'].mean()}\")\n",
    "print(f\" The median of the Br spectral line width is {df['Br'].median()}\")\n",
    "print(f\" The first quantile of the Br spectral line width is {df['Br'].quantile(0.25)}\")\n",
    "print(f\" The thrid quantile of the Br spectral line width is {df['Br'].quantile(0.75)}\")\n",
    "print(f\" The variance of the Br spectral line width is {df['Br'].var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try sns.pairplot(train_tab, hue=\"Type\")\n",
    "sns.pairplot(df, hue = \"Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-milan",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1ade9527f4911bfa8369e8dfa1ac77b",
     "grade": false,
     "grade_id": "cell-7608858d462435ee",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Step 4: Prepare the data for ML algorithms\n",
    "\n",
    "Machine learning algorithms often require data to be pre-processed in a certain way, such as scaling numerical values or map categories to other representations. You will also need to decide what data to use as training and as testing. You may also need to develop a strategy to deal with missing data. \n",
    "\n",
    "For this lab, we have already done the feature engineering to get the equivalent widths, their uncertainties, and the correlation coefficients. \n",
    "\n",
    "## Question 12\n",
    "(6 pts)\n",
    "\n",
    "While the features are already chosen, there are a number of other questions that need to be answered:\n",
    "\n",
    "<b style=\"color:red\">1. Are there bad data points or outliers?   \n",
    "2. If there are outliers, what is your strategy for dealing with them?   \n",
    "3. Apply your strategy for cleaning and dealing with the data, then split your training data into three parts: training, testing, and validation  </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dc93b-5b79-47fa-ac04-ffd9d4f4c713",
   "metadata": {},
   "source": [
    "1) Yes, there are bad data points in the dataset. For example, there are data points where the Br emission lines are negative or in the tens of thousands.\n",
    "2) Due to the large mean value of the Br emission line, I plan on using interquartile ranges to eliminate values. To clean the data, I will remove any data point beyond 1.5 * IQR past the first and third quartiles, as per the IQR rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "be34a024-5060-4b7e-a414-b696712c3d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Na  Na_err  Corr    Br  Br_err  Yng_corr    SNR  Type\n",
       "0    1.11    0.91  0.81  2.57    0.53      0.54  17.69     1\n",
       "1    5.40    0.78  0.77  0.98    0.26      0.36  20.74     1\n",
       "2    5.71    0.80  0.81  0.96    0.22      0.34  20.23     1\n",
       "3    2.54    1.45  0.93  2.94    0.87      0.11  10.92     1\n",
       "5    5.11    0.71  0.85  0.79    0.18      0.28  22.79     1\n",
       "..    ...     ...   ...   ...     ...       ...    ...   ...\n",
       "407  1.06    0.81  0.72 -1.06    0.27      0.18  19.93     0\n",
       "413  0.70    0.73  0.53  3.68    0.49      0.64  22.15     0\n",
       "414  0.16    0.59  0.44  1.26    0.25      0.48  27.61     0\n",
       "415  1.78    0.68  0.74  1.01    0.22      0.49  23.98     0\n",
       "418  0.74    1.04  0.18  3.26    0.87      0.42  15.39     0\n",
       "\n",
       "[257 rows x 8 columns]>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df\n",
    "for feat in df.select_dtypes(include = 'number').columns:\n",
    "    Q1 = df[feat].quantile(0.25)\n",
    "    Q3 = df[feat].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    low = Q1 - 1.5 * IQR\n",
    "    high = Q3 + 1.5 *IQR\n",
    "    df_new = df_new[(df_new[feat] >= low) & (df_new[feat] <= high)]\n",
    "#df_new = df.drop(outlier.index)\n",
    "\n",
    "df_new['Type'] = df_new['Type'].apply(lambda x: 0 if x == 'young' else 1)\n",
    "df_new.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "north-lover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Na</th>\n",
       "      <th>Na_err</th>\n",
       "      <th>Corr</th>\n",
       "      <th>Br</th>\n",
       "      <th>Br_err</th>\n",
       "      <th>Yng_corr</th>\n",
       "      <th>SNR</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>257.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>257.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.465253</td>\n",
       "      <td>0.929105</td>\n",
       "      <td>0.699728</td>\n",
       "      <td>1.984086</td>\n",
       "      <td>0.519844</td>\n",
       "      <td>0.418872</td>\n",
       "      <td>18.950156</td>\n",
       "      <td>0.817121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.905164</td>\n",
       "      <td>0.273731</td>\n",
       "      <td>0.154812</td>\n",
       "      <td>1.765167</td>\n",
       "      <td>0.320190</td>\n",
       "      <td>0.225118</td>\n",
       "      <td>5.915087</td>\n",
       "      <td>0.387322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.050000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>-3.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>8.920000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.230000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>14.920000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.830000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.900000</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.440000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>1.670000</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>37.650000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Na      Na_err        Corr          Br      Br_err    Yng_corr  \\\n",
       "count  257.000000  257.000000  257.000000  257.000000  257.000000  257.000000   \n",
       "mean     3.465253    0.929105    0.699728    1.984086    0.519844    0.418872   \n",
       "std      1.905164    0.273731    0.154812    1.765167    0.320190    0.225118   \n",
       "min     -1.050000    0.440000    0.180000   -3.050000    0.140000   -0.200000   \n",
       "25%      2.230000    0.730000    0.590000    0.960000    0.260000    0.270000   \n",
       "50%      3.830000    0.900000    0.720000    1.640000    0.460000    0.380000   \n",
       "75%      4.900000    1.070000    0.830000    3.040000    0.670000    0.540000   \n",
       "max      7.440000    1.750000    0.940000    7.330000    1.670000    1.240000   \n",
       "\n",
       "              SNR        Type  \n",
       "count  257.000000  257.000000  \n",
       "mean    18.950156    0.817121  \n",
       "std      5.915087    0.387322  \n",
       "min      8.920000    0.000000  \n",
       "25%     14.920000    1.000000  \n",
       "50%     17.800000    1.000000  \n",
       "75%     22.200000    1.000000  \n",
       "max     37.650000    1.000000  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe your process, clean the data, and show clean_training.describe()\n",
    "\"Data was cleaned using the 1.5 IQR Rule to elimiante outliers. Outliers 1.5 * IQR above the 3rd quartile and 1.5 * IRQ below the first quartile in any column\"  \n",
    "\"besides type were removed from the data set.\"\n",
    "df_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before splitting the training data into trainig, testing, and validation sets,\n",
    "# plot clean training with sns.pairplot(clean_training, hue=\"Type\")\n",
    "sns.pairplot(df, hue=\"Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "public-client",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d97d9bbb16299283c7a2963f0a4a0a",
     "grade": true,
     "grade_id": "cell-f8b403f8c6cec921",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# After cleaning, split data into training, testing, and validation\n",
    "data = df_new.drop('Type', axis = 1)\n",
    "x_train_all, x_validate, y_train_all, y_validate = model_selection.train_test_split(data, df_new['Type'], test_size=0.1)\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train_all, y_train_all, test_size=0.2)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-threat",
   "metadata": {},
   "source": [
    "# Step 5: Select a model and train it\n",
    "\n",
    "Based on the problem and the data, there are often a handful of algorithms to try. Here, experimentation and knowledge of the strengths and weaknesses of machine learning models will help you choose a model and train it.\n",
    "\n",
    "\n",
    "## Question 13\n",
    "(10 pts)\n",
    "\n",
    "In this lab, we will specifically use the Naive Bayes model. \n",
    "\n",
    "In the following cells, please implement the Naive Bayes classifier to deterimine whether a star is young or old based on the features you've selected in Step 4.  \n",
    "<b style=\"color:red\">At the end of this step, report the accuracy of your model on the test data.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ordinary-guard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18478260869565222, 0.8152173913043478]\n"
     ]
    }
   ],
   "source": [
    "# use as many cells as you need to do Step 5. We will be grading manually\n",
    "features = x_train[['Na', 'Br']]\n",
    "tgt = pd.DataFrame({'Type': y_train})\n",
    "galactic_combined = pd.concat([tgt, features], axis = 1)\n",
    "\n",
    "p1 = sum(y_train)\n",
    "tot = len(y_train)\n",
    "\n",
    "priors = [1 - (p1 / tot), (p1 / tot)]\n",
    "print(priors)\n",
    "\n",
    "g_n0_mean, g_n0_var = g_parameters(galactic_combined, 0, \"Na\", 'Type')\n",
    "g_n1_mean, g_n1_var = g_parameters(galactic_combined, 1, \"Na\", 'Type')\n",
    "g_b0_mean, g_b0_var = g_parameters(galactic_combined, 0, \"Br\", 'Type')\n",
    "g_b1_mean, g_b1_var = g_parameters(galactic_combined, 1, \"Br\", 'Type')\n",
    "parameters = [[g_n0_mean, g_n0_var, g_b0_mean, g_b0_var], [g_n1_mean, g_n1_var, g_b1_mean, g_b1_var]]\n",
    "\n",
    "pred = []\n",
    "labels = ['young', 'old']\n",
    "\n",
    "for x in x_test.to_numpy():\n",
    "    curr = -1\n",
    "    curr_tgt = -1\n",
    "    for y in range (0, 2):\n",
    "        tmp = priors[y] * gaussian(parameters[y][0], parameters[y][1], x[0]) * gaussian(parameters[y][2], parameters[y][3], x[1])\n",
    "        if tmp > curr: # Updates max score and most likely target\n",
    "            curr_tgt = y\n",
    "            curr = tmp\n",
    "    pred.append(curr_tgt) # Adds prediction, then resets model for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "tight-calcium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574468085106383"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment these and run when ready\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-opera",
   "metadata": {},
   "source": [
    "# Step 6: Fine tune the model\n",
    "\n",
    "Most machine learning algorithms will have hyperparameters that can be tuned to improve the performance of the model given the specific dataset. You may also want to visualize the results of model predictions to aid in tuning the model.\n",
    "\n",
    "\n",
    "## Question 14\n",
    "(9 pts)\n",
    "\n",
    "For this lab, please do the following:  \n",
    "<b style=\"color:red\">\n",
    "1. One of the hyperparameters in this case is the types of features to use. Try subtracting or adding a feature to see how it changes your accuracy.   \n",
    "2. Examine how changes in your training data might affect your results. For example, what if you use only measurements with small uncertainties for training?   \n",
    "3. Report your final accuracy on your test dataset after selecting the optimal model. Explain why you think this is the optimal model.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "36c6bf96-aba7-40e2-bdf4-841ea10efe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = 1000 # Make as many runs as possible to make an average guess on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "consecutive-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9697234042553248\n"
     ]
    }
   ],
   "source": [
    "# use as many cells as you need to do Step 6. We will be grading manually\n",
    "# 3 Feature Model (Na, Br emission lines, and Na Correlation)\n",
    "avg = 0\n",
    "\n",
    "features = x_train[['Na', 'Br', 'Corr']] # Takes features we care about\n",
    "tgt = pd.DataFrame({'Type': y_train}) \n",
    "galactic_combined = pd.concat([tgt, features], axis = 1)\n",
    "    \n",
    "p1 = sum(y_train) # Due to precomputing all old == 1 and young == 0, we can simply sum the y_train dataset to find the number of old classifications\n",
    "tot = len(y_train) # Total\n",
    "    \n",
    "priors = [1 - (p1 / tot), (p1 / tot)] # Priors in terms of percentages\n",
    "\n",
    "#---Gaussian Parameter Calculations---\n",
    "params = [] \n",
    "for y in range(0, 2): # 2 classes -> range (0, 2)\n",
    "    curr = []\n",
    "    for x in galactic_combined: # runs over our dataframe with the notable features and types, we are running over every feature\n",
    "        if x == 'Type': # we do not want to compute gaussian parameters for the type\n",
    "            continue\n",
    "        mean, var = g_parameters(galactic_combined, y, x, 'Type') # Finds mean and var by running through the parameters function \n",
    "        curr.append(mean) \n",
    "        curr.append(var)\n",
    "    params.append(curr) # [[mean_feature1_class1, var_feature1_class1, mean_feature2_class1, ...], [mean_feature1_class2, ...], ...]\n",
    "        \n",
    "for i in range(0, tests):\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train_all, y_train_all, test_size=0.2) # Obtain a new set of testing data\n",
    "    \n",
    "    pred = []\n",
    "    \n",
    "    for x in range (0, len(x_test)): # Find the maximum of the argumet: prior * gaussian distribution of feature 1 * gaussian distribution of feature 2 * ...\n",
    "        curr = -1\n",
    "        curr_tgt = -1\n",
    "        for y in range (0, 2):\n",
    "            tmp = priors[y] * gaussian(params[y][0], params[y][1], x_test.iloc[x].Na) * gaussian(params[y][2], params[y][3], x_test.iloc[x].Br) * gaussian(params[y][4], params[y][5], x_test.iloc[x].Corr)\n",
    "            if tmp > curr: # Updates max score and most likely target\n",
    "                curr_tgt = y\n",
    "                curr = tmp\n",
    "        pred.append(curr_tgt) # Adds prediction, then resets model for the next iteration\n",
    "    avg += accuracy_score(y_test, pred) # add accuracy ([0, 1]) to avg\n",
    "print(avg / tests) # divide by total number of tests to find average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ac47825c-d991-45dc-b379-b5aa3d4db607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9745319148936225\n"
     ]
    }
   ],
   "source": [
    "# 2 Feature Model (Na, Br emission lines)\n",
    "avg = 0\n",
    "features = x_train[['Na', 'Br']]\n",
    "tgt = pd.DataFrame({'Type': y_train})\n",
    "galactic_combined = pd.concat([tgt, features], axis = 1)\n",
    "    \n",
    "p1 = sum(y_train)\n",
    "tot = len(y_train)\n",
    "    \n",
    "priors = [1 - (p1 / tot), (p1 / tot)]\n",
    "    \n",
    "params = []\n",
    "for y in range(0, 2):\n",
    "    curr = []\n",
    "    for x in galactic_combined:\n",
    "        if x == 'Type':\n",
    "            continue\n",
    "        mean, var = g_parameters(galactic_combined, y, x, 'Type')\n",
    "        curr.append(mean)\n",
    "        curr.append(var)\n",
    "    params.append(curr)\n",
    "    \n",
    "for i in range (0, tests):\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train_all, y_train_all, test_size=0.2)\n",
    "    \n",
    "    pred = []\n",
    "    labels = ['young', 'old']\n",
    "    \n",
    "    for x in range (0, len(x_test)):\n",
    "        curr = -1\n",
    "        curr_tgt = -1\n",
    "        for y in range (0, 2):\n",
    "            tmp = priors[y] * gaussian(params[y][0], params[y][1], x_test.iloc[x].Na) * gaussian(params[y][2], params[y][3], x_test.iloc[x].Br)\n",
    "            if tmp > curr: # Updates max score and most likely target\n",
    "                curr_tgt = y\n",
    "                curr = tmp\n",
    "        pred.append(curr_tgt) # Adds prediction, then resets model for the next iteration\n",
    "    avg += accuracy_score(y_test, pred)\n",
    "print(avg / tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9da8fb89-a779-44e1-aaf0-480b90662609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9941212121212158\n"
     ]
    }
   ],
   "source": [
    "# Small Error Model\n",
    "avg = 0\n",
    "\n",
    "#---Dataframe prepation--- \n",
    "df_err = df # Working with original dataset\n",
    "remove = pd.DataFrame()\n",
    "for i in range (0, len(df)): # For the features we are using, remove any error > 37%\n",
    "    if (df_err.iloc[i].Na != 0 and abs(df_err.iloc[i].Na_err / df_err.iloc[i].Na) > 0.37): \n",
    "        remove = pd.concat([remove, df_err.iloc[i].to_frame().T])\n",
    "for i in range (0, len(df)):\n",
    "    if (df_err.iloc[i].Br != 0 and abs(df_err.iloc[i].Br_err / df_err.iloc[i].Br) > 0.37): \n",
    "        remove = pd.concat([remove, df_err.iloc[i].to_frame().T])\n",
    "        \n",
    "df_err = df_err.drop(remove.index)\n",
    "df_err['Type'] = df_err['Type'].apply(lambda x: 0 if x == 'young' else 1)\n",
    "\n",
    "data = df_err.drop('Type', axis = 1)\n",
    "\n",
    "x_train_all, x_validate, y_train_all, y_validate = model_selection.train_test_split(data, df_err['Type'], test_size=0.1)\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train_all, y_train_all, test_size=0.2)\n",
    "\n",
    "#---Prior Calculation---\n",
    "features = x_train[['Na', 'Br']]\n",
    "tgt = pd.DataFrame({'Type': y_train})\n",
    "galactic_combined = pd.concat([tgt, features], axis = 1)\n",
    "    \n",
    "p1 = sum(y_train)\n",
    "tot = len(y_train)\n",
    "    \n",
    "priors = [1 - (p1 / tot), (p1 / tot)]\n",
    "params = []\n",
    "\n",
    "#---Gaussian Parameter Calculation---\n",
    "for y in range(0, 2):\n",
    "    curr = []\n",
    "    for x in galactic_combined:\n",
    "        if x == 'Type':\n",
    "            continue\n",
    "        mean, var = g_parameters(galactic_combined, y, x, 'Type')\n",
    "        curr.append(mean)\n",
    "        curr.append(var)\n",
    "    params.append(curr)\n",
    "\n",
    "#---Testing---\n",
    "for i in range (0, tests):\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train_all, y_train_all, test_size=0.2)\n",
    "    \n",
    "    pred = []\n",
    "\n",
    "    for x in range(0, len(x_test)):\n",
    "        curr = -1\n",
    "        curr_tgt = -1\n",
    "        for y in range (0, 2):\n",
    "            tmp = priors[y] * gaussian(params[y][0], params[y][1], x_test.iloc[x].Na) * gaussian(params[y][2], params[y][3], x_test.iloc[x].Br)\n",
    "            if tmp > curr: # Updates max score and most likely target\n",
    "                curr_tgt = y\n",
    "                curr = tmp\n",
    "        pred.append(curr_tgt) # Adds prediction, then resets model for the next iteration\n",
    "    avg += accuracy_score(y_test, pred)\n",
    "print(avg / tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c69858-0b93-4429-83c3-56d9d98f9e87",
   "metadata": {},
   "source": [
    "1) Generally, adding more features seems to decrease the accuracy of the model\n",
    "2) Small Error model was created above\n",
    "3) Best Score: Average of 99.41% correctness from Small Error (>37% error eliminated) based on 1000 total tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-orientation",
   "metadata": {},
   "source": [
    "# Step 7: Present your solution and apply it\n",
    "\n",
    "Here you will evaluate your model against the test set that was set aside in Step 2 to determine the final performance metrics. Your model is now ready to be applied to new data.\n",
    "\n",
    "## Question 15\n",
    "(2 pts)\n",
    "\n",
    "For this lab, please do the following:  \n",
    "<b style=\"color:red\">\n",
    "1. Report your accuracy for the validation data that you saved in Step 4. How does this score compare to when you were training and fine tuning your model?  \n",
    "2. Apply your model to the data for the unlabeled grading data. Note that you should explore these inputs to see if there are differences in the features between your training data and this dataset. Beware of outliers in this dataset as well. Create a set of predictions called pred_eval.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "comparable-basement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use as many cells as you need to do Step 7. \n",
    "df_err = df\n",
    "remove = pd.DataFrame()\n",
    "for i in range (0, len(df)):\n",
    "    if (df_err.iloc[i].Na != 0 and abs(df_err.iloc[i].Na_err / df_err.iloc[i].Na) > 0.37):\n",
    "        remove = pd.concat([remove, df_err.iloc[i].to_frame().T])\n",
    "for i in range (0, len(df)):\n",
    "    if (df_err.iloc[i].Br != 0 and abs(df_err.iloc[i].Br_err / df_err.iloc[i].Br) > 0.37): \n",
    "        remove = pd.concat([remove, df_err.iloc[i].to_frame().T])\n",
    "\n",
    "df_err = df_err.drop(remove.index)\n",
    "df_err['Type'] = df_err['Type'].apply(lambda x: 0 if x == 'young' else 1)\n",
    "\n",
    "data = df_err.drop('Type', axis = 1)\n",
    "\n",
    "x_train_all, x_validate, y_train_all, y_validate = model_selection.train_test_split(data, df_err['Type'], test_size=0.1)\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train_all, y_train_all, test_size=0.2)\n",
    "\n",
    "features = x_train[['Na', 'Br']]\n",
    "tgt = pd.DataFrame({'Type': y_train})\n",
    "galactic_combined = pd.concat([tgt, features], axis = 1)\n",
    "    \n",
    "p1 = sum(y_train)\n",
    "tot = len(y_train)\n",
    "    \n",
    "priors = [1 - (p1 / tot), (p1 / tot)]\n",
    "params = []\n",
    "for y in range(0, 2):\n",
    "    curr = []\n",
    "    for x in galactic_combined:\n",
    "        if x == 'Type':\n",
    "            continue\n",
    "        mean, var = g_parameters(galactic_combined, y, x, 'Type')\n",
    "        curr.append(mean)\n",
    "        curr.append(var)\n",
    "    params.append(curr)\n",
    "    \n",
    "pred = []\n",
    "labels = ['young', 'old']\n",
    "    \n",
    "for x in range(0, len(x_validate)):\n",
    "    curr = -1\n",
    "    curr_tgt = -1\n",
    "    for y in range (0, 2):\n",
    "        tmp = priors[y] * gaussian(params[y][0], params[y][1], x_validate.iloc[x].Na) * gaussian(params[y][2], params[y][3], x_validate.iloc[x].Br)\n",
    "        if tmp > curr: # Updates max score and most likely target\n",
    "            curr_tgt = y\n",
    "            curr = tmp\n",
    "    pred.append(curr_tgt) # Adds prediction, then resets model for the next iteration\n",
    "accuracy_score(y_validate, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-waste",
   "metadata": {},
   "source": [
    "If you would like to submit your group's solution to the unlabeled dataset for the competition to see who gets the accurate predictions, please send your TA and Prof. Do a csv file with a single column predicting whether each star is 'young' or 'old'. Below is an example code of how to create this file. You will send ONE file per team. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "interested-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# if your answer for three stars were:\n",
    "#answer = ['old','young','old']\n",
    "\n",
    "\n",
    "path_comp = \"galactic_center_stars_eval.csv\"\n",
    "df_comp = pd.read_csv(path_comp)\n",
    "\n",
    "for x in range (0, len(df_comp)):\n",
    "    curr = -1\n",
    "    curr_tgt = -1\n",
    "    for y in range (0, 2):\n",
    "        tmp = priors[y] * gaussian(params[y][0], params[y][1], df_comp.iloc[x].Na) * gaussian(params[y][2], params[y][3], df_comp.iloc[x].Br)\n",
    "        if tmp > curr: # Updates max score and most likely target\n",
    "            curr_tgt = y\n",
    "            curr = tmp\n",
    "    pred.append(curr_tgt) # Adds prediction, then resets model for the next iteration\n",
    "\n",
    "answer = []\n",
    "for x in pred:\n",
    "    if x == 0:\n",
    "        answer.append('young')\n",
    "    else:\n",
    "        answer.append('old')\n",
    "\n",
    "# change the list into a data frame\n",
    "answer_tab = pd.DataFrame({'target':answer})\n",
    "\n",
    "# save the data frame into a csv file\n",
    "answer_tab.to_csv('group_x_answer.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9416f9-a773-4fa1-aa0e-26cd201effdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
